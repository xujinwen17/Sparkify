{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "404c1a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addPyFile(\"/Users/xujinwen/spark/spark-3.3.1-bin-hadoop2/jars/graphframes-0.8.2-spark3.0-s_2.12.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3169f1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from graphframes import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed1ff49c",
   "metadata": {},
   "source": [
    "MongoDB Spark Connect\n",
    "\n",
    "Step1: Connect to MongoDB instances\\\n",
    "Step2: Start Spark Shell from the command line\n",
    "    \n",
    "    pyspark --conf \"spark.mongodb.read.connection.uri=mongodb://127.0.0.1/test.sparkify1200\" \\\n",
    "              --conf \"spark.mongodb.write.connection.uri=mongodb://127.0.0.1/test.sparkify1200\" \\\n",
    "              --packages org.mongodb.spark:mongo-spark-connector_2.12:10.1.1\n",
    "\n",
    "Step3: Create a SparkSession Object, specified the spark.mongodb.input.uri and spark.mongodb.output.uri configuration options. Use SparkSession.builder and specify different configuration options to create your own SparkSession object.\n",
    "\n",
    "- The spark.mongodb.input.uri specifies the MongoDB server address (127.0.0.1), the database to connect (test), and the collection (myCollection) from which to read data, and the read preference.\n",
    "- The spark.mongodb.output.uri specifies the MongoDB server address (127.0.0.1), the database to connect (test), and the collection (myCollection) to which to write data. Connects to port 27017 by default.\n",
    "- The packages option specifies the Spark Connector's Maven coordinates, in the format groupId:artifactId:version.\n",
    "\n",
    "-> Uncomment below code to use MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6c9090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName(\"myApp\") \\\n",
    "#     .config(\"spark.mongodb.input.uri\", \"mongodb://@127.0.0.1:27017/test.sparkify1200\") \\\n",
    "#     .config(\"spark.mongodb.output.uri\", \"mongodb://@127.0.0.1:27017/test.sparkify1200\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# #load data\n",
    "# data = spark.read.format(\"mongodb\").option(\"header\",\"true\").option('escape','\"').load()\n",
    "# original_count = data.count()\n",
    "# data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "482095df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/29 15:08:32 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify_Network\") \\\n",
    "    .getOrCreate()\n",
    "# Set time parser policy\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc2a8d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "path = \"data/mini_sparkify_event_data.json\"\n",
    "data = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5987f709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import size, collect_set\n",
    "df = data.select(\"artist\", \"userId\") \\\n",
    "                    .groupBy(\"artist\") \\\n",
    "                    .agg(collect_set(col('userId')).alias('user_list')) \\\n",
    "                    .select('*',size('user_list').alias('size')) \\\n",
    "                    .filter(\"size>=150\") \\\n",
    "                    .filter(\"artist != 'null'\") \\\n",
    "                    .orderBy(\"size\", ascending = False)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb8ef091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+\n",
      "|              artist|           user_list|size|\n",
      "+--------------------+--------------------+----+\n",
      "|       Kings Of Leon|[120, 66, 300013,...| 199|\n",
      "|       Dwight Yoakam|[120, 66, 300013,...| 189|\n",
      "|            Coldplay|[120, 66, 141, 15...| 189|\n",
      "|Florence + The Ma...|[120, 66, 300013,...| 187|\n",
      "|            BjÃÂ¶rk|[120, 66, 300013,...| 179|\n",
      "|      The Black Keys|[120, 66, 300013,...| 179|\n",
      "|       Justin Bieber|[120, 66, 141, 15...| 177|\n",
      "|        Taylor Swift|[120, 66, 141, 15...| 173|\n",
      "|        Jack Johnson|[120, 66, 300013,...| 173|\n",
      "|     Alliance Ethnik|[66, 120, 300013,...| 172|\n",
      "|            Harmonia|[120, 66, 15, 154...| 172|\n",
      "|       Guns N' Roses|[120, 66, 300013,...| 170|\n",
      "|               Train|[120, 66, 141, 15...| 169|\n",
      "|              Eminem|[120, 66, 141, 15...| 169|\n",
      "|         The Killers|[120, 66, 141, 15...| 168|\n",
      "|         OneRepublic|[66, 120, 300013,...| 168|\n",
      "|           Metallica|[66, 120, 300013,...| 168|\n",
      "|           Radiohead|[120, 66, 300013,...| 167|\n",
      "|                Muse|[120, 66, 300013,...| 166|\n",
      "|          John Mayer|[120, 66, 300013,...| 166|\n",
      "+--------------------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c674d615",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|                  id|size|\n",
      "+--------------------+----+\n",
      "|       Kings Of Leon| 199|\n",
      "|       Dwight Yoakam| 189|\n",
      "|            Coldplay| 189|\n",
      "|Florence + The Ma...| 187|\n",
      "|            BjÃÂ¶rk| 179|\n",
      "|      The Black Keys| 179|\n",
      "|       Justin Bieber| 177|\n",
      "|        Taylor Swift| 173|\n",
      "|        Jack Johnson| 173|\n",
      "|     Alliance Ethnik| 172|\n",
      "|            Harmonia| 172|\n",
      "|       Guns N' Roses| 170|\n",
      "|               Train| 169|\n",
      "|              Eminem| 169|\n",
      "|         The Killers| 168|\n",
      "|         OneRepublic| 168|\n",
      "|           Metallica| 168|\n",
      "|           Radiohead| 167|\n",
      "|                Muse| 166|\n",
      "|          John Mayer| 166|\n",
      "+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "song_vertices = df.select(\"artist\", \"size\").withColumnRenamed(\"artist\",\"id\")\n",
    "song_vertices.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19541adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/29 15:11:55 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "23/05/29 15:11:55 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "23/05/29 15:11:55 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "23/05/29 15:11:55 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "23/05/29 15:11:57 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "+-------------+--------------------+----------------+\n",
      "|      artist1|             artist2|common_listeners|\n",
      "+-------------+--------------------+----------------+\n",
      "|Kings Of Leon|            Coldplay|             181|\n",
      "|Kings Of Leon|       Dwight Yoakam|             181|\n",
      "|Kings Of Leon|Florence + The Ma...|             179|\n",
      "|Kings Of Leon|      The Black Keys|             176|\n",
      "|Kings Of Leon|            BjÃÂ¶rk|             172|\n",
      "|Kings Of Leon|       Justin Bieber|             173|\n",
      "|Kings Of Leon|        Taylor Swift|             170|\n",
      "|Kings Of Leon|        Jack Johnson|             167|\n",
      "|Kings Of Leon|            Harmonia|             166|\n",
      "|Kings Of Leon|     Alliance Ethnik|             167|\n",
      "|Kings Of Leon|       Guns N' Roses|             162|\n",
      "|Kings Of Leon|              Eminem|             164|\n",
      "|Kings Of Leon|               Train|             163|\n",
      "|Kings Of Leon|         The Killers|             162|\n",
      "|Kings Of Leon|           Metallica|             164|\n",
      "|Kings Of Leon|         OneRepublic|             165|\n",
      "|Kings Of Leon|           Radiohead|             161|\n",
      "|Kings Of Leon|          John Mayer|             162|\n",
      "|Kings Of Leon|         Evanescence|             161|\n",
      "|Kings Of Leon|                Muse|             163|\n",
      "+-------------+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 4020:=============>                                       (25 + 8) / 100]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 获取艺术家列表\n",
    "artists = df.select(\"artist\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# 定义结果数据框架的模式\n",
    "result_schema = StructType([\n",
    "    StructField(\"artist1\", StringType(), True),\n",
    "    StructField(\"artist2\", StringType(), True),\n",
    "    StructField(\"common_listeners\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# 创建空的结果数据框架\n",
    "result = spark.createDataFrame([], result_schema)\n",
    "\n",
    "# 遍历艺术家列表并找到共同的听众列表\n",
    "for i in range(len(artists)):\n",
    "    for j in range(i + 1, len(artists)):\n",
    "        artist1 = artists[i]\n",
    "        artist2 = artists[j]\n",
    "        listeners1 = set(df.filter(col(\"artist\") == artist1).select(\"user_list\").first()[0])\n",
    "        listeners2 = set(df.filter(col(\"artist\") == artist2).select(\"user_list\").first()[0])\n",
    "        common_listeners = len(listeners1.intersection(listeners2))\n",
    "        if common_listeners > 0:\n",
    "                result = result.union(spark.createDataFrame([(artist1, artist2, common_listeners)], [\"artist1\", \"artist2\", \"common_listeners\"]))\n",
    "\n",
    "\n",
    "# 显示结果\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a967f5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/29 15:13:45 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "23/05/29 15:13:45 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "23/05/29 15:13:46 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "23/05/29 15:13:46 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "23/05/29 15:13:47 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "+-------------+--------------------+------+\n",
      "|          src|                 dst|weight|\n",
      "+-------------+--------------------+------+\n",
      "|Kings Of Leon|            Coldplay|   181|\n",
      "|Kings Of Leon|       Dwight Yoakam|   181|\n",
      "|Kings Of Leon|Florence + The Ma...|   179|\n",
      "|Kings Of Leon|      The Black Keys|   176|\n",
      "|Kings Of Leon|            BjÃÂ¶rk|   172|\n",
      "|Kings Of Leon|       Justin Bieber|   173|\n",
      "|Kings Of Leon|        Taylor Swift|   170|\n",
      "|Kings Of Leon|        Jack Johnson|   167|\n",
      "|Kings Of Leon|            Harmonia|   166|\n",
      "|Kings Of Leon|     Alliance Ethnik|   167|\n",
      "|Kings Of Leon|       Guns N' Roses|   162|\n",
      "|Kings Of Leon|              Eminem|   164|\n",
      "|Kings Of Leon|               Train|   163|\n",
      "|Kings Of Leon|         The Killers|   162|\n",
      "|Kings Of Leon|           Metallica|   164|\n",
      "|Kings Of Leon|         OneRepublic|   165|\n",
      "|Kings Of Leon|           Radiohead|   161|\n",
      "|Kings Of Leon|          John Mayer|   162|\n",
      "|Kings Of Leon|         Evanescence|   161|\n",
      "|Kings Of Leon|                Muse|   163|\n",
      "+-------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "song_edges = result.toDF(\"src\",\"dst\",\"weight\")\n",
    "song_edges.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50a6474c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/29 15:14:10 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "edges = song_edges.toPandas()\n",
    "\n",
    "# 导出为 CSV 文件\n",
    "edges.to_csv(\"edges.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cace2f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xujinwen/spark/spark-3.3.1-bin-hadoop2/python/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "song_graph = GraphFrame(song_vertices, song_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6e3cd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 4028:==============>                                         (2 + 6) / 8]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_graph.vertices.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfecf240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xujinwen/spark/spark-3.3.1-bin-hadoop2/python/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/29 17:43:43 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4034:=================================================>(5334 + 2) / 5336]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/29 17:44:31 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "+--------------------+---------+\n",
      "|                  id|outDegree|\n",
      "+--------------------+---------+\n",
      "|       Kings Of Leon|       36|\n",
      "|            Coldplay|       35|\n",
      "|       Dwight Yoakam|       34|\n",
      "|Florence + The Ma...|       33|\n",
      "|      The Black Keys|       32|\n",
      "|            BjÃÂ¶rk|       31|\n",
      "|       Justin Bieber|       30|\n",
      "|        Taylor Swift|       29|\n",
      "|        Jack Johnson|       28|\n",
      "|            Harmonia|       27|\n",
      "|     Alliance Ethnik|       26|\n",
      "|       Guns N' Roses|       25|\n",
      "|              Eminem|       24|\n",
      "|               Train|       23|\n",
      "|         The Killers|       22|\n",
      "|           Metallica|       21|\n",
      "|         OneRepublic|       20|\n",
      "|           Radiohead|       19|\n",
      "|          John Mayer|       18|\n",
      "|         Evanescence|       17|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "song_graph.outDegrees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230ecabd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
